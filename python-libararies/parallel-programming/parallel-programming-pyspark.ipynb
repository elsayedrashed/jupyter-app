{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Specify additional jars for Spark jobs\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_jars = \"../jars/*\"\n",
    "\n",
    "spark_packages = [\n",
    "    'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2',\n",
    "    'org.apache.kafka:kafka-clients:3.2.3'\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Dataframe using a JDBC Connection\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", spark_jars) \\\n",
    "    .config(\"spark.executor.extraClassPath\", spark_jars) \\\n",
    "    .config(\"spark.jars.packages\", \",\".join(spark_packages)) \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "+-----------+--------------------+\n",
      "|        key|               value|\n",
      "+-----------+--------------------+\n",
      "|17179869184|{\"InvoiceNo\":\"498...|\n",
      "|51539607552|{\"InvoiceNo\":\"518...|\n",
      "|77309411328|{\"InvoiceNo\":\"532...|\n",
      "|34359738368|{\"InvoiceNo\":\"508...|\n",
      "|68719476736|{\"InvoiceNo\":\"527...|\n",
      "|85899345920|{\"InvoiceNo\":\"536...|\n",
      "|77309411329|{\"InvoiceNo\":\"532...|\n",
      "|34359738369|{\"InvoiceNo\":\"508...|\n",
      "|85899345921|{\"InvoiceNo\":\"536...|\n",
      "|25769803779|{\"InvoiceNo\":\"503...|\n",
      "|17179869186|{\"InvoiceNo\":\"498...|\n",
      "|60129542147|{\"InvoiceNo\":\"523...|\n",
      "|25769803780|{\"InvoiceNo\":\"503...|\n",
      "|          4|{\"InvoiceNo\":\"489...|\n",
      "|68719476739|{\"InvoiceNo\":\"527...|\n",
      "|77309411331|{\"InvoiceNo\":\"532...|\n",
      "|34359738372|{\"InvoiceNo\":\"508...|\n",
      "|17179869187|{\"InvoiceNo\":\"498...|\n",
      "|68719476740|{\"InvoiceNo\":\"527...|\n",
      "|25769803781|{\"InvoiceNo\":\"503...|\n",
      "+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_df = spark.read.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"topic_data\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"kafka.group.id\", \"myConsumerGroup\")\\\n",
    "    .load()\n",
    "\n",
    "kafka_df.printSchema()\n",
    "kafka_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+-------+------+----------+-----+-----------------+\n",
      "|         name| age| income|gender|department|grade|performance_score|\n",
      "+-------------+----+-------+------+----------+-----+-----------------+\n",
      "|  Allen Smith|45.0|    NaN|   NaN|Operations|   G3|              723|\n",
      "|      S Kumar| NaN|16000.0|     F|   Finance|   G0|              520|\n",
      "|  Jack Morgan|32.0|35000.0|     M|   Finance|   G2|              674|\n",
      "|    Ying Chin|45.0|65000.0|     F|     Sales|   G3|              556|\n",
      "|Dheeraj Patel|30.0|42000.0|     F|Operations|   G2|              711|\n",
      "|Satyam Sharma| NaN|62000.0|   NaN|     Sales|   G3|              649|\n",
      "| James Authur|54.0|    NaN|     F|Operations|   G3|               53|\n",
      "|   Josh Wills|54.0|52000.0|     F|   Finance|   G3|              901|\n",
      "|     Leo Duck|23.0|98000.0|     M|     Sales|   G4|              709|\n",
      "+-------------+----+-------+------+----------+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retail_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .load(\"../data/employee.csv\")\n",
    "\n",
    "retail_df.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# MySQl JDBC Connection\n",
    "df_mysql = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/spark_labs\") \\\n",
    "    .option(\"dbtable\", \"ch02\") \\\n",
    "    .option(\"user\", \"dbuser\") \\\n",
    "    .option(\"password\", \"dbuser\") \\\n",
    "    .option(\"useSSL\", \"false\") \\\n",
    "    .load()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------------+\n",
      "|   lname|         fname|                name|\n",
      "+--------+--------------+--------------------+\n",
      "|   Karau|        Holden|       Karau, Holden|\n",
      "|Maréchal|Pierre Sylvain|Maréchal, Pierre ...|\n",
      "|  Pascal|        Blaise|      Pascal, Blaise|\n",
      "|  Perrin|  Jean-Georges|Perrin, Jean-Georges|\n",
      "|Voltaire|      François|  Voltaire, François|\n",
      "+--------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- lname: string (nullable = true)\n",
      " |-- fname: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mysql = df_mysql.orderBy(df_mysql.lname)\n",
    "\n",
    "# Displays the dataframe and some of its metadata\n",
    "df_mysql.show(5)\n",
    "df_mysql.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# PostgresSQL JDBC Connection\n",
    "df_postgres = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/spark_labs\") \\\n",
    "    .option(\"dbtable\", \"ch02\") \\\n",
    "    .option(\"user\", \"dbuser\") \\\n",
    "    .option(\"password\", \"dbuser\") \\\n",
    "    .option(\"useSSL\", \"false\") \\\n",
    "    .load()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------------+\n",
      "|   lname|         fname|                name|\n",
      "+--------+--------------+--------------------+\n",
      "|   Karau|        Holden|       Karau, Holden|\n",
      "|Maréchal|Pierre Sylvain|Maréchal, Pierre ...|\n",
      "|  Pascal|        Blaise|      Pascal, Blaise|\n",
      "|  Perrin|  Jean-Georges|Perrin, Jean-Georges|\n",
      "|Voltaire|      François|  Voltaire, François|\n",
      "+--------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- lname: string (nullable = true)\n",
      " |-- fname: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_postgres = df_postgres.orderBy(df_postgres.lname)\n",
    "\n",
    "# Displays the dataframe and some of its metadata\n",
    "df_postgres.show(5)\n",
    "df_postgres.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# MySQl JDBC Connection\n",
    "df_mysql = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/dbase\") \\\n",
    "    .option(\"dbtable\", \"tweet\") \\\n",
    "    .option(\"user\", \"dbuser\") \\\n",
    "    .option(\"password\", \"dbuser\") \\\n",
    "    .option(\"useSSL\", \"false\") \\\n",
    "    .load()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tweet_json: string (nullable = true)\n",
      " |-- format: string (nullable = true)\n",
      " |-- tweet_id: string (nullable = true)\n",
      " |-- actor_display_name: string (nullable = true)\n",
      " |-- actor_followers_count: integer (nullable = true)\n",
      " |-- actor_friends_count: integer (nullable = true)\n",
      " |-- actor_id: string (nullable = true)\n",
      " |-- actor_status_count: integer (nullable = true)\n",
      " |-- actor_preferred_username: string (nullable = true)\n",
      " |-- actor_image_url: string (nullable = true)\n",
      " |-- actor_posted_time: timestamp (nullable = true)\n",
      " |-- verb: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- hashtag_list: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- is_fake_tweet: integer (nullable = true)\n",
      " |-- in_reply_to: string (nullable = true)\n",
      " |-- posted_time: timestamp (nullable = true)\n",
      " |-- retweet_count: integer (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- generator_display_name: string (nullable = true)\n",
      " |-- contains_mention: integer (nullable = true)\n",
      " |-- object_actor_display_name: string (nullable = true)\n",
      " |-- object_actor_followers_count: integer (nullable = true)\n",
      " |-- object_actor_friends_count: integer (nullable = true)\n",
      " |-- object_actor_id: string (nullable = true)\n",
      " |-- object_actor_status_count: integer (nullable = true)\n",
      " |-- object_actor_preferred_username: string (nullable = true)\n",
      " |-- object_actor_posted_time: timestamp (nullable = true)\n",
      " |-- original_tweet_id: string (nullable = true)\n",
      " |-- workspace_list: string (nullable = true)\n",
      " |-- rule_list: string (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- location_geo_coordinates: string (nullable = true)\n",
      " |-- actor_listed_count: integer (nullable = true)\n",
      " |-- actor_favorites_count: integer (nullable = true)\n",
      " |-- favorites_count: integer (nullable = true)\n",
      " |-- object_posted_time: timestamp (nullable = true)\n",
      " |-- in_ministries: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- creation_time: timestamp (nullable = true)\n",
      " |-- is_spam: string (nullable = true)\n",
      " |-- is_pro_isis: string (nullable = true)\n",
      " |-- quoted_tweet_id: string (nullable = true)\n",
      " |-- quoted_body: string (nullable = true)\n",
      " |-- quoted_actor_id: string (nullable = true)\n",
      " |-- quoted_actor_display_name: string (nullable = true)\n",
      " |-- quoted_actor_preferred_username: string (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- actor_summary: string (nullable = true)\n",
      " |-- location_twitter_country_code: string (nullable = true)\n",
      " |-- actor_location_country_code: string (nullable = true)\n",
      " |-- tweet_link: string (nullable = true)\n",
      " |-- actor_banner_url: string (nullable = true)\n",
      " |-- is_verified: string (nullable = true)\n",
      " |-- in_reply_to_actor_id: string (nullable = true)\n",
      " |-- object_actor_image_url: string (nullable = true)\n",
      " |-- quoted_actor_image_url: string (nullable = true)\n",
      " |-- in_reply_to_actor_preferred_username: string (nullable = true)\n",
      " |-- in_reply_to_tweet_id: string (nullable = true)\n",
      " |-- is_deleted_from_twitter: string (nullable = true)\n",
      " |-- deleted_time: timestamp (nullable = true)\n",
      " |-- is_observed: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mysql.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "raw_df = spark.read.format(\"image\") \\\n",
    "    .load('../data/images/')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "image_df = raw_df.select(\"image.origin\", \"image.height\",\"image.width\", \"image.nChannels\", \"image.mode\", \"image.data\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "csv_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .load(\"../data/online_retail_II.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "525461"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_df.count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   489434|    85048|15CM CHRISTMAS GL...|      12|01/12/09 07:45|     6.95|     13085|United Kingdom|\n",
      "|   489434|   79323P|  PINK CHERRY LIGHTS|      12|01/12/09 07:45|     6.75|     13085|United Kingdom|\n",
      "|   489434|   79323W| WHITE CHERRY LIGHTS|      12|01/12/09 07:45|     6.75|     13085|United Kingdom|\n",
      "|   489434|    22041|\"RECORD FRAME 7\"\"...|      48|01/12/09 07:45|      2.1|     13085|United Kingdom|\n",
      "|   489434|    21232|STRAWBERRY CERAMI...|      24|01/12/09 07:45|     1.25|     13085|United Kingdom|\n",
      "|   489434|    22064|PINK DOUGHNUT TRI...|      24|01/12/09 07:45|     1.65|     13085|United Kingdom|\n",
      "|   489434|    21871| SAVE THE PLANET MUG|      24|01/12/09 07:45|     1.25|     13085|United Kingdom|\n",
      "|   489434|    21523|FANCY FONT HOME S...|      10|01/12/09 07:45|     5.95|     13085|United Kingdom|\n",
      "|   489435|    22350|           CAT BOWL |      12|01/12/09 07:46|     2.55|     13085|United Kingdom|\n",
      "|   489435|    22349|DOG BOWL , CHASIN...|      12|01/12/09 07:46|     3.75|     13085|United Kingdom|\n",
      "|   489435|    22195|HEART MEASURING S...|      24|01/12/09 07:46|     1.65|     13085|United Kingdom|\n",
      "|   489435|    22353|LUNCHBOX WITH CUT...|      12|01/12/09 07:46|     2.55|     13085|United Kingdom|\n",
      "|   489436|   48173C|DOOR MAT BLACK FL...|      10|01/12/09 09:06|     5.95|     13078|United Kingdom|\n",
      "|   489436|    21755|LOVE BUILDING BLO...|      18|01/12/09 09:06|     5.45|     13078|United Kingdom|\n",
      "|   489436|    21754|HOME BUILDING BLO...|       3|01/12/09 09:06|     5.95|     13078|United Kingdom|\n",
      "|   489436|    84879|ASSORTED COLOUR B...|      16|01/12/09 09:06|     1.69|     13078|United Kingdom|\n",
      "|   489436|    22119| PEACE WOODEN BLO...|       3|01/12/09 09:06|     6.95|     13078|United Kingdom|\n",
      "|   489436|    22142|CHRISTMAS CRAFT W...|      12|01/12/09 09:06|     1.45|     13078|United Kingdom|\n",
      "|   489436|    22296|HEART IVORY TRELL...|      12|01/12/09 09:06|     1.65|     13078|United Kingdom|\n",
      "|   489436|    22295|HEART FILIGREE DO...|      12|01/12/09 09:06|     1.65|     13078|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retail_df = csv_df.selectExpr(\"Invoice as InvoiceNo\", \"StockCode\", \"Description\", \"Quantity\", \"InvoiceDate\", \"Price as UnitPrice\", \"`Customer ID` as CustomerID\", \"Country\")\n",
    "retail_df.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|key|               value|\n",
      "+---+--------------------+\n",
      "|  0|{\"InvoiceNo\":\"489...|\n",
      "|  1|{\"InvoiceNo\":\"489...|\n",
      "|  2|{\"InvoiceNo\":\"489...|\n",
      "|  3|{\"InvoiceNo\":\"489...|\n",
      "|  4|{\"InvoiceNo\":\"489...|\n",
      "|  5|{\"InvoiceNo\":\"489...|\n",
      "|  6|{\"InvoiceNo\":\"489...|\n",
      "|  7|{\"InvoiceNo\":\"489...|\n",
      "|  8|{\"InvoiceNo\":\"489...|\n",
      "|  9|{\"InvoiceNo\":\"489...|\n",
      "| 10|{\"InvoiceNo\":\"489...|\n",
      "| 11|{\"InvoiceNo\":\"489...|\n",
      "| 12|{\"InvoiceNo\":\"489...|\n",
      "| 13|{\"InvoiceNo\":\"489...|\n",
      "| 14|{\"InvoiceNo\":\"489...|\n",
      "| 15|{\"InvoiceNo\":\"489...|\n",
      "| 16|{\"InvoiceNo\":\"489...|\n",
      "| 17|{\"InvoiceNo\":\"489...|\n",
      "| 18|{\"InvoiceNo\":\"489...|\n",
      "| 19|{\"InvoiceNo\":\"489...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert data record ro JSON message\n",
    "from pyspark.sql.functions import to_json, struct, from_json, monotonically_increasing_id\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "kafka_df = retail_df.withColumn(\"key\", monotonically_increasing_id().cast(\"STRING\")).withColumn(\"value\", to_json(struct([retail_df[x] for x in retail_df.columns])).cast(\"STRING\"))\n",
    "\n",
    "jsonSchema = StructType([ StructField(\"eventName\", StringType(), True), StructField(\"eventParams\", StringType(), True)])\n",
    "kafka_df.select(\"key\", \"value\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "kafka_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    .write \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"topic_data\") \\\n",
    "    .save()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Data source\n",
    "retail_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .load(\"../data/online_retail_II.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Save to MySQl JDBC Connection\n",
    "retail_df.write.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/dbase\") \\\n",
    "    .option(\"dbtable\", \"online_retail_II\") \\\n",
    "    .option(\"user\", \"dbuser\") \\\n",
    "    .option(\"password\", \"dbuser\") \\\n",
    "    .option(\"useSSL\", \"false\") \\\n",
    "    .save()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "retail_df.write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"../data/retail.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "retail_df.write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"../data/retail/\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "retail_df.write.mode(\"overwrite\") \\\n",
    "    .parquet(\"../data/retail-parquet/\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "retail_df.write.mode(\"overwrite\") \\\n",
    "    .orc(\"../data/retail-orc/\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType\n",
    "from pyspark.sql.functions import col, from_json, to_date\n",
    "\n",
    "eventSchema = StructType()\\\n",
    "    .add('InvoiceNo', StringType())\\\n",
    "    .add('StockCode', StringType())\\\n",
    "    .add('Description', StringType())\\\n",
    "    .add('Quantity', IntegerType())\\\n",
    "    .add('InvoiceDate', StringType())\\\n",
    "    .add('UnitPrice', DoubleType())\\\n",
    "    .add('CustomerID', IntegerType())\\\n",
    "    .add('Country', StringType())\n",
    "\n",
    "retail_df = kafka_df.select(from_json(col(\"value\"). cast(StringType()), eventSchema).alias(\"message\"), col(\"timestamp\").alias(\"EventTime\")).select(\"message.*\", \"EventTime\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- EventTime: timestamp (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+--------------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|           EventTime|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+--------------------+\n",
      "|   498903|    22384|LUNCH BAG PINK RE...|      10|23/02/10 14:16|     1.65|     14112|United Kingdom|2023-03-08 02:13:...|\n",
      "|   518776|    82600|NO SINGING METAL ...|      12|11/08/10 15:49|      2.1|     15110|United Kingdom|2023-03-08 02:13:...|\n",
      "|   532351|    22411|JUMBO SHOPPER VIN...|       2|11/11/10 16:29|     1.95|     14379|United Kingdom|2023-03-08 02:13:...|\n",
      "|   508747|    22087|PAPER BUNTING WHI...|       1|18/05/10 11:40|     5.91|      null|United Kingdom|2023-03-08 02:13:...|\n",
      "|   527928|    22426|ENAMEL WASH BOWL ...|       4|19/10/10 13:28|     3.75|     13488|United Kingdom|2023-03-08 02:13:...|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retail_df.printSchema()\n",
    "retail_df.show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|[7B 22 49 6E 76 6...|\n",
      "|[7B 22 49 6E 76 6...|\n",
      "|[7B 22 49 6E 76 6...|\n",
      "|[7B 22 49 6E 76 6...|\n",
      "|[7B 22 49 6E 76 6...|\n",
      "|[7B 22 49 6E 76 6...|\n",
      "|[7B 22 49 6E 76 6...|\n",
      "|[7B 22 49 6E 76 6...|\n",
      "|[7B 22 49 6E 76 6...|\n",
      "|[7B 22 49 6E 76 6...|\n",
      "|[7B 22 49 6E 76 6...|\n",
      "|[7B 22 49 6E 76 6...|\n",
      "|[7B 22 49 6E 76 6...|\n",
      "|[7B 22 49 6E 76 6...|\n",
      "|[7B 22 49 6E 76 6...|\n",
      "|[7B 22 49 6E 76 6...|\n",
      "|[7B 22 49 6E 76 6...|\n",
      "|[7B 22 49 6E 76 6...|\n",
      "|[7B 22 49 6E 76 6...|\n",
      "|[7B 22 49 6E 76 6...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "path can be only string, list or RDD",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-21-301a24f636d2>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mdf1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkafka_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"value\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdf2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mdf2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.pyenv/versions/env3.10.8/lib/python3.10/site-packages/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mjson\u001B[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001B[0m\n\u001B[1;32m    299\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjrdd\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    300\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 301\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"path can be only string, list or RDD\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    303\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mtable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtableName\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;34m\"DataFrame\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: path can be only string, list or RDD"
     ]
    }
   ],
   "source": [
    "df1 = kafka_df.select(\"value\").show()\n",
    "df2 = spark.read.json(df1)\n",
    "df2.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "Kafka_stream_df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"topic_data\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"kafka.group.id\", \"streamConsumerGroup\") \\\n",
    "    .load()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Kafka_stream_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query = Kafka_stream_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()#%%\n",
    "# Specify additional jars for Spark jobs\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_jars = \"../jars/*\"\n",
    "\n",
    "spark_packages = [\n",
    "    'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2',\n",
    "    'org.apache.kafka:kafka-clients:3.2.3'\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Dataframe using a JDBC Connection\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", spark_jars) \\\n",
    "    .config(\"spark.executor.extraClassPath\", spark_jars) \\\n",
    "    .config(\"spark.jars.packages\", \",\".join(spark_packages)) \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kafka_df = spark.read.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"topic_data\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"kafka.group.id\", \"myConsumerGroup\")\\\n",
    "    .load()\n",
    "\n",
    "kafka_df.printSchema()\n",
    "kafka_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retail_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .load(\"../data/employee.csv\")\n",
    "\n",
    "retail_df.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# MySQl JDBC Connection\n",
    "df_mysql = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/spark_labs\") \\\n",
    "    .option(\"dbtable\", \"ch02\") \\\n",
    "    .option(\"user\", \"dbuser\") \\\n",
    "    .option(\"password\", \"dbuser\") \\\n",
    "    .option(\"useSSL\", \"false\") \\\n",
    "    .load()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_mysql = df_mysql.orderBy(df_mysql.lname)\n",
    "\n",
    "# Displays the dataframe and some of its metadata\n",
    "df_mysql.show(5)\n",
    "df_mysql.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PostgresSQL JDBC Connection\n",
    "df_postgres = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/spark_labs\") \\\n",
    "    .option(\"dbtable\", \"ch02\") \\\n",
    "    .option(\"user\", \"dbuser\") \\\n",
    "    .option(\"password\", \"dbuser\") \\\n",
    "    .option(\"useSSL\", \"false\") \\\n",
    "    .load()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_postgres = df_postgres.orderBy(df_postgres.lname)\n",
    "\n",
    "# Displays the dataframe and some of its metadata\n",
    "df_postgres.show(5)\n",
    "df_postgres.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# MySQl JDBC Connection\n",
    "df_mysql = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/dbase\") \\\n",
    "    .option(\"dbtable\", \"tweet\") \\\n",
    "    .option(\"user\", \"dbuser\") \\\n",
    "    .option(\"password\", \"dbuser\") \\\n",
    "    .option(\"useSSL\", \"false\") \\\n",
    "    .load()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_mysql.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_df = spark.read.format(\"image\") \\\n",
    "    .load('../data/images/')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_df = raw_df.select(\"image.origin\", \"image.height\",\"image.width\", \"image.nChannels\", \"image.mode\", \"image.data\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "csv_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .load(\"../data/online_retail_II.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "csv_df.count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retail_df = csv_df.selectExpr(\"Invoice as InvoiceNo\", \"StockCode\", \"Description\", \"Quantity\", \"InvoiceDate\", \"Price as UnitPrice\", \"`Customer ID` as CustomerID\", \"Country\")\n",
    "retail_df.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert data record ro JSON message\n",
    "from pyspark.sql.functions import to_json, struct, from_json, monotonically_increasing_id\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "kafka_df = retail_df.withColumn(\"key\", monotonically_increasing_id().cast(\"STRING\")).withColumn(\"value\", to_json(struct([retail_df[x] for x in retail_df.columns])).cast(\"STRING\"))\n",
    "\n",
    "jsonSchema = StructType([ StructField(\"eventName\", StringType(), True), StructField(\"eventParams\", StringType(), True)])\n",
    "kafka_df.select(\"key\", \"value\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kafka_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    .write \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"topic_data\") \\\n",
    "    .save()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data source\n",
    "retail_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .load(\"../data/online_retail_II.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save to MySQl JDBC Connection\n",
    "retail_df.write.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/dbase\") \\\n",
    "    .option(\"dbtable\", \"online_retail_II\") \\\n",
    "    .option(\"user\", \"dbuser\") \\\n",
    "    .option(\"password\", \"dbuser\") \\\n",
    "    .option(\"useSSL\", \"false\") \\\n",
    "    .save()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retail_df.write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"../data/retail.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retail_df.write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"../data/retail/\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retail_df.write.mode(\"overwrite\") \\\n",
    "    .parquet(\"../data/retail-parquet/\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retail_df.write.mode(\"overwrite\") \\\n",
    "    .orc(\"../data/retail-orc/\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType\n",
    "from pyspark.sql.functions import col, from_json, to_date\n",
    "\n",
    "eventSchema = StructType()\\\n",
    "    .add('InvoiceNo', StringType())\\\n",
    "    .add('StockCode', StringType())\\\n",
    "    .add('Description', StringType())\\\n",
    "    .add('Quantity', IntegerType())\\\n",
    "    .add('InvoiceDate', StringType())\\\n",
    "    .add('UnitPrice', DoubleType())\\\n",
    "    .add('CustomerID', IntegerType())\\\n",
    "    .add('Country', StringType())\n",
    "\n",
    "retail_df = kafka_df.select(from_json(col(\"value\"). cast(StringType()), eventSchema).alias(\"message\"), col(\"timestamp\").alias(\"EventTime\")).select(\"message.*\", \"EventTime\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retail_df.printSchema()\n",
    "retail_df.show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df1 = kafka_df.select(\"value\").show()\n",
    "df2 = spark.read.json(df1)\n",
    "df2.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Kafka_stream_df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"topic_data\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"kafka.group.id\", \"streamConsumerGroup\") \\\n",
    "    .load()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Kafka_stream_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query = Kafka_stream_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"testk1\") \\\n",
    "    .start()#%%\n",
    "# Specify additional jars for Spark jobs\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_jars = \"../jars/*\"\n",
    "\n",
    "spark_packages = [\n",
    "    'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2',\n",
    "    'org.apache.kafka:kafka-clients:3.2.3'\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Dataframe using a JDBC Connection\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", spark_jars) \\\n",
    "    .config(\"spark.executor.extraClassPath\", spark_jars) \\\n",
    "    .config(\"spark.jars.packages\", \",\".join(spark_packages)) \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kafka_df = spark.read.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"topic_data\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"kafka.group.id\", \"myConsumerGroup\")\\\n",
    "    .load()\n",
    "\n",
    "kafka_df.printSchema()\n",
    "kafka_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retail_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .load(\"../data/employee.csv\")\n",
    "\n",
    "retail_df.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# MySQl JDBC Connection\n",
    "df_mysql = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/spark_labs\") \\\n",
    "    .option(\"dbtable\", \"ch02\") \\\n",
    "    .option(\"user\", \"dbuser\") \\\n",
    "    .option(\"password\", \"dbuser\") \\\n",
    "    .option(\"useSSL\", \"false\") \\\n",
    "    .load()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_mysql = df_mysql.orderBy(df_mysql.lname)\n",
    "\n",
    "# Displays the dataframe and some of its metadata\n",
    "df_mysql.show(5)\n",
    "df_mysql.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PostgresSQL JDBC Connection\n",
    "df_postgres = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/spark_labs\") \\\n",
    "    .option(\"dbtable\", \"ch02\") \\\n",
    "    .option(\"user\", \"dbuser\") \\\n",
    "    .option(\"password\", \"dbuser\") \\\n",
    "    .option(\"useSSL\", \"false\") \\\n",
    "    .load()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_postgres = df_postgres.orderBy(df_postgres.lname)\n",
    "\n",
    "# Displays the dataframe and some of its metadata\n",
    "df_postgres.show(5)\n",
    "df_postgres.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# MySQl JDBC Connection\n",
    "df_mysql = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/dbase\") \\\n",
    "    .option(\"dbtable\", \"tweet\") \\\n",
    "    .option(\"user\", \"dbuser\") \\\n",
    "    .option(\"password\", \"dbuser\") \\\n",
    "    .option(\"useSSL\", \"false\") \\\n",
    "    .load()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_mysql.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_df = spark.read.format(\"image\") \\\n",
    "    .load('../data/images/')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_df = raw_df.select(\"image.origin\", \"image.height\",\"image.width\", \"image.nChannels\", \"image.mode\", \"image.data\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "csv_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .load(\"../data/online_retail_II.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "csv_df.count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retail_df = csv_df.selectExpr(\"Invoice as InvoiceNo\", \"StockCode\", \"Description\", \"Quantity\", \"InvoiceDate\", \"Price as UnitPrice\", \"`Customer ID` as CustomerID\", \"Country\")\n",
    "retail_df.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert data record ro JSON message\n",
    "from pyspark.sql.functions import to_json, struct, from_json, monotonically_increasing_id\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "kafka_df = retail_df.withColumn(\"key\", monotonically_increasing_id().cast(\"STRING\")).withColumn(\"value\", to_json(struct([retail_df[x] for x in retail_df.columns])).cast(\"STRING\"))\n",
    "\n",
    "jsonSchema = StructType([ StructField(\"eventName\", StringType(), True), StructField(\"eventParams\", StringType(), True)])\n",
    "kafka_df.select(\"key\", \"value\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kafka_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    .write \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"topic_data\") \\\n",
    "    .save()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data source\n",
    "retail_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .load(\"../data/online_retail_II.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save to MySQl JDBC Connection\n",
    "retail_df.write.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/dbase\") \\\n",
    "    .option(\"dbtable\", \"online_retail_II\") \\\n",
    "    .option(\"user\", \"dbuser\") \\\n",
    "    .option(\"password\", \"dbuser\") \\\n",
    "    .option(\"useSSL\", \"false\") \\\n",
    "    .save()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retail_df.write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"../data/retail.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retail_df.write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"../data/retail/\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retail_df.write.mode(\"overwrite\") \\\n",
    "    .parquet(\"../data/retail-parquet/\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retail_df.write.mode(\"overwrite\") \\\n",
    "    .orc(\"../data/retail-orc/\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType\n",
    "from pyspark.sql.functions import col, from_json, to_date\n",
    "\n",
    "eventSchema = StructType()\\\n",
    "    .add('InvoiceNo', StringType())\\\n",
    "    .add('StockCode', StringType())\\\n",
    "    .add('Description', StringType())\\\n",
    "    .add('Quantity', IntegerType())\\\n",
    "    .add('InvoiceDate', StringType())\\\n",
    "    .add('UnitPrice', DoubleType())\\\n",
    "    .add('CustomerID', IntegerType())\\\n",
    "    .add('Country', StringType())\n",
    "\n",
    "retail_df = kafka_df.select(from_json(col(\"value\"). cast(StringType()), eventSchema).alias(\"message\"), col(\"timestamp\").alias(\"EventTime\")).select(\"message.*\", \"EventTime\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retail_df.printSchema()\n",
    "retail_df.show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df1 = kafka_df.select(\"value\").show()\n",
    "df2 = spark.read.json(df1)\n",
    "df2.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Kafka_stream_df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"topic_data\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"kafka.group.id\", \"streamConsumerGroup\") \\\n",
    "    .load()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Kafka_stream_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query = Kafka_stream_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()#%%\n",
    "# Specify additional jars for Spark jobs\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_jars = \"../jars/*\"\n",
    "\n",
    "spark_packages = [\n",
    "    'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2',\n",
    "    'org.apache.kafka:kafka-clients:3.2.3'\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Dataframe using a JDBC Connection\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", spark_jars) \\\n",
    "    .config(\"spark.executor.extraClassPath\", spark_jars) \\\n",
    "    .config(\"spark.jars.packages\", \",\".join(spark_packages)) \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kafka_df = spark.read.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"topic_data\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"kafka.group.id\", \"myConsumerGroup\")\\\n",
    "    .load()\n",
    "\n",
    "kafka_df.printSchema()\n",
    "kafka_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retail_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .load(\"../data/employee.csv\")\n",
    "\n",
    "retail_df.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# MySQl JDBC Connection\n",
    "df_mysql = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/spark_labs\") \\\n",
    "    .option(\"dbtable\", \"ch02\") \\\n",
    "    .option(\"user\", \"dbuser\") \\\n",
    "    .option(\"password\", \"dbuser\") \\\n",
    "    .option(\"useSSL\", \"false\") \\\n",
    "    .load()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_mysql = df_mysql.orderBy(df_mysql.lname)\n",
    "\n",
    "# Displays the dataframe and some of its metadata\n",
    "df_mysql.show(5)\n",
    "df_mysql.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PostgresSQL JDBC Connection\n",
    "df_postgres = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/spark_labs\") \\\n",
    "    .option(\"dbtable\", \"ch02\") \\\n",
    "    .option(\"user\", \"dbuser\") \\\n",
    "    .option(\"password\", \"dbuser\") \\\n",
    "    .option(\"useSSL\", \"false\") \\\n",
    "    .load()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_postgres = df_postgres.orderBy(df_postgres.lname)\n",
    "\n",
    "# Displays the dataframe and some of its metadata\n",
    "df_postgres.show(5)\n",
    "df_postgres.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# MySQl JDBC Connection\n",
    "df_mysql = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/dbase\") \\\n",
    "    .option(\"dbtable\", \"tweet\") \\\n",
    "    .option(\"user\", \"dbuser\") \\\n",
    "    .option(\"password\", \"dbuser\") \\\n",
    "    .option(\"useSSL\", \"false\") \\\n",
    "    .load()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_mysql.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_df = spark.read.format(\"image\") \\\n",
    "    .load('../data/images/')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_df = raw_df.select(\"image.origin\", \"image.height\",\"image.width\", \"image.nChannels\", \"image.mode\", \"image.data\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "csv_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .load(\"../data/online_retail_II.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "csv_df.count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retail_df = csv_df.selectExpr(\"Invoice as InvoiceNo\", \"StockCode\", \"Description\", \"Quantity\", \"InvoiceDate\", \"Price as UnitPrice\", \"`Customer ID` as CustomerID\", \"Country\")\n",
    "retail_df.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert data record ro JSON message\n",
    "from pyspark.sql.functions import to_json, struct, from_json, monotonically_increasing_id\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "kafka_df = retail_df.withColumn(\"key\", monotonically_increasing_id().cast(\"STRING\")).withColumn(\"value\", to_json(struct([retail_df[x] for x in retail_df.columns])).cast(\"STRING\"))\n",
    "\n",
    "jsonSchema = StructType([ StructField(\"eventName\", StringType(), True), StructField(\"eventParams\", StringType(), True)])\n",
    "kafka_df.select(\"key\", \"value\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "kafka_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    .write \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"topic_data\") \\\n",
    "    .save()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data source\n",
    "retail_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .load(\"../data/online_retail_II.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save to MySQl JDBC Connection\n",
    "retail_df.write.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/dbase\") \\\n",
    "    .option(\"dbtable\", \"online_retail_II\") \\\n",
    "    .option(\"user\", \"dbuser\") \\\n",
    "    .option(\"password\", \"dbuser\") \\\n",
    "    .option(\"useSSL\", \"false\") \\\n",
    "    .save()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retail_df.write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"../data/retail.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retail_df.write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"../data/retail/\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retail_df.write.mode(\"overwrite\") \\\n",
    "    .parquet(\"../data/retail-parquet/\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retail_df.write.mode(\"overwrite\") \\\n",
    "    .orc(\"../data/retail-orc/\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType\n",
    "from pyspark.sql.functions import col, from_json, to_date\n",
    "\n",
    "eventSchema = StructType()\\\n",
    "    .add('InvoiceNo', StringType())\\\n",
    "    .add('StockCode', StringType())\\\n",
    "    .add('Description', StringType())\\\n",
    "    .add('Quantity', IntegerType())\\\n",
    "    .add('InvoiceDate', StringType())\\\n",
    "    .add('UnitPrice', DoubleType())\\\n",
    "    .add('CustomerID', IntegerType())\\\n",
    "    .add('Country', StringType())\n",
    "\n",
    "retail_df = kafka_df.select(from_json(col(\"value\"). cast(StringType()), eventSchema).alias(\"message\"), col(\"timestamp\").alias(\"EventTime\")).select(\"message.*\", \"EventTime\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retail_df.printSchema()\n",
    "retail_df.show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df1 = kafka_df.select(\"value\").show()\n",
    "df2 = spark.read.json(df1)\n",
    "df2.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "Kafka_stream_df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"topic_data\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"kafka.group.id\", \"streamConsumerGroup\") \\\n",
    "    .load()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "|query = Kafka_stream_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"testk1\") \\\n",
    "    .start()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.awaitTermination(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "query.stop()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.status"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|        key|               value|\n",
      "+-----------+--------------------+\n",
      "|25769803776|{\"InvoiceNo\":\"503...|\n",
      "|          0|{\"InvoiceNo\":\"489...|\n",
      "|60129542144|{\"InvoiceNo\":\"523...|\n",
      "| 8589934592|{\"InvoiceNo\":\"494...|\n",
      "|42949672961|{\"InvoiceNo\":\"513...|\n",
      "|25769803777|{\"InvoiceNo\":\"503...|\n",
      "|17179869185|{\"InvoiceNo\":\"498...|\n",
      "|68719476737|{\"InvoiceNo\":\"527...|\n",
      "| 8589934593|{\"InvoiceNo\":\"494...|\n",
      "|25769803778|{\"InvoiceNo\":\"503...|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query data\n",
    "query_result=spark.sql(\"SELECT * from testk1 LIMIT 10\")\n",
    "query_result.show(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   cnt|\n",
      "+------+\n",
      "|525461|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) AS cnt FROM testk1\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@7e4a9e25, org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy$$Lambda$4993/1319113060@323da47d\n",
      "+- *(1) Project [cast(key#511 as string) AS key#834, cast(value#512 as string) AS value#835]\n",
      "   +- MicroBatchScan[key#511, value#512, topic#513, partition#514, offset#515L, timestamp#516, timestampType#517] class org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query.explain()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'id': 'eda1097a-edfc-4b40-aa11-1bcaa5267bd2',\n  'runId': 'cfa7b9e7-69f8-4252-ab39-77df0c3a5cbb',\n  'name': 'testk1',\n  'timestamp': '2023-03-08T11:34:58.003Z',\n  'batchId': 0,\n  'numInputRows': 525461,\n  'inputRowsPerSecond': 0.0,\n  'processedRowsPerSecond': 96414.8623853211,\n  'durationMs': {'addBatch': 5170,\n   'getBatch': 1,\n   'latestOffset': 125,\n   'queryPlanning': 7,\n   'triggerExecution': 5450,\n   'walCommit': 74},\n  'stateOperators': [],\n  'sources': [{'description': 'KafkaV2[Subscribe[topic_data]]',\n    'startOffset': None,\n    'endOffset': {'topic_data': {'2': 175458, '1': 175334, '0': 174669}},\n    'latestOffset': {'topic_data': {'2': 175458, '1': 175334, '0': 174669}},\n    'numInputRows': 525461,\n    'inputRowsPerSecond': 0.0,\n    'processedRowsPerSecond': 96414.8623853211,\n    'metrics': {'avgOffsetsBehindLatest': '0.0',\n     'maxOffsetsBehindLatest': '0',\n     'minOffsetsBehindLatest': '0'}}],\n  'sink': {'description': 'MemorySink', 'numOutputRows': 525461}},\n {'id': 'eda1097a-edfc-4b40-aa11-1bcaa5267bd2',\n  'runId': 'cfa7b9e7-69f8-4252-ab39-77df0c3a5cbb',\n  'name': 'testk1',\n  'timestamp': '2023-03-08T11:35:13.452Z',\n  'batchId': 1,\n  'numInputRows': 0,\n  'inputRowsPerSecond': 0.0,\n  'processedRowsPerSecond': 0.0,\n  'durationMs': {'latestOffset': 2, 'triggerExecution': 2},\n  'stateOperators': [],\n  'sources': [{'description': 'KafkaV2[Subscribe[topic_data]]',\n    'startOffset': {'topic_data': {'2': 175458, '1': 175334, '0': 174669}},\n    'endOffset': {'topic_data': {'2': 175458, '1': 175334, '0': 174669}},\n    'latestOffset': {'topic_data': {'2': 175458, '1': 175334, '0': 174669}},\n    'numInputRows': 0,\n    'inputRowsPerSecond': 0.0,\n    'processedRowsPerSecond': 0.0,\n    'metrics': {'avgOffsetsBehindLatest': '0.0',\n     'maxOffsetsBehindLatest': '0',\n     'minOffsetsBehindLatest': '0'}}],\n  'sink': {'description': 'MemorySink', 'numOutputRows': 0}},\n {'id': 'eda1097a-edfc-4b40-aa11-1bcaa5267bd2',\n  'runId': 'cfa7b9e7-69f8-4252-ab39-77df0c3a5cbb',\n  'name': 'testk1',\n  'timestamp': '2023-03-08T11:35:23.463Z',\n  'batchId': 1,\n  'numInputRows': 0,\n  'inputRowsPerSecond': 0.0,\n  'processedRowsPerSecond': 0.0,\n  'durationMs': {'latestOffset': 1, 'triggerExecution': 1},\n  'stateOperators': [],\n  'sources': [{'description': 'KafkaV2[Subscribe[topic_data]]',\n    'startOffset': {'topic_data': {'2': 175458, '1': 175334, '0': 174669}},\n    'endOffset': {'topic_data': {'2': 175458, '1': 175334, '0': 174669}},\n    'latestOffset': {'topic_data': {'2': 175458, '1': 175334, '0': 174669}},\n    'numInputRows': 0,\n    'inputRowsPerSecond': 0.0,\n    'processedRowsPerSecond': 0.0,\n    'metrics': {'avgOffsetsBehindLatest': '0.0',\n     'maxOffsetsBehindLatest': '0',\n     'minOffsetsBehindLatest': '0'}}],\n  'sink': {'description': 'MemorySink', 'numOutputRows': 0}}]"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.recentProgress"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "{'id': 'eda1097a-edfc-4b40-aa11-1bcaa5267bd2',\n 'runId': 'cfa7b9e7-69f8-4252-ab39-77df0c3a5cbb',\n 'name': 'testk1',\n 'timestamp': '2023-03-08T11:35:23.463Z',\n 'batchId': 1,\n 'numInputRows': 0,\n 'inputRowsPerSecond': 0.0,\n 'processedRowsPerSecond': 0.0,\n 'durationMs': {'latestOffset': 1, 'triggerExecution': 1},\n 'stateOperators': [],\n 'sources': [{'description': 'KafkaV2[Subscribe[topic_data]]',\n   'startOffset': {'topic_data': {'2': 175458, '1': 175334, '0': 174669}},\n   'endOffset': {'topic_data': {'2': 175458, '1': 175334, '0': 174669}},\n   'latestOffset': {'topic_data': {'2': 175458, '1': 175334, '0': 174669}},\n   'numInputRows': 0,\n   'inputRowsPerSecond': 0.0,\n   'processedRowsPerSecond': 0.0,\n   'metrics': {'avgOffsetsBehindLatest': '0.0',\n    'maxOffsetsBehindLatest': '0',\n    'minOffsetsBehindLatest': '0'}}],\n 'sink': {'description': 'MemorySink', 'numOutputRows': 0}}"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.lastProgress"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "Query [id = 63f4bebd-e656-4ca8-9341-4e5c50523d32, runId = dd6ca762-342d-43ec-af7a-a5fb2f012e6e] terminated with exception: Set(topic_data-2) are gone. Kafka option 'kafka.group.id' has been set on this query, it is\n not recommended to set this option. This option is unsafe to use since multiple concurrent\n queries or sources using the same group id will interfere with each other as they are part\n of the same consumer group. Restarted queries may also suffer interference from the\n previous run having the same group id. The user should have only one query per group id,\n and/or set the option 'kafka.session.timeout.ms' to be very small so that the Kafka\n consumers from the previous query are marked dead by the Kafka group coordinator before the\n restarted query starts running.\n    . \nSome data may have been lost because they are not available in Kafka any more; either the\n data was aged out by Kafka or the topic may have been deleted before all the data in the\n topic was processed. If you don't want your streaming query to fail on such cases, set the\n source option \"failOnDataLoss\" to \"false\".\n    ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mStreamingQueryException\u001B[0m                   Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-57-269cde5c647b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstreams\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mawaitAnyTermination\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/.pyenv/versions/env3.10.8/lib/python3.10/site-packages/pyspark/sql/streaming.py\u001B[0m in \u001B[0;36mawaitAnyTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    289\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsqm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mawaitAnyTermination\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;36m1000\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    290\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 291\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsqm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mawaitAnyTermination\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    292\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    293\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mresetTerminated\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.pyenv/versions/env3.10.8/lib/python3.10/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.pyenv/versions/env3.10.8/lib/python3.10/site-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    194\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    195\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 196\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    197\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mStreamingQueryException\u001B[0m: Query [id = 63f4bebd-e656-4ca8-9341-4e5c50523d32, runId = dd6ca762-342d-43ec-af7a-a5fb2f012e6e] terminated with exception: Set(topic_data-2) are gone. Kafka option 'kafka.group.id' has been set on this query, it is\n not recommended to set this option. This option is unsafe to use since multiple concurrent\n queries or sources using the same group id will interfere with each other as they are part\n of the same consumer group. Restarted queries may also suffer interference from the\n previous run having the same group id. The user should have only one query per group id,\n and/or set the option 'kafka.session.timeout.ms' to be very small so that the Kafka\n consumers from the previous query are marked dead by the Kafka group coordinator before the\n restarted query starts running.\n    . \nSome data may have been lost because they are not available in Kafka any more; either the\n data was aged out by Kafka or the topic may have been deleted before all the data in the\n topic was processed. If you don't want your streaming query to fail on such cases, set the\n source option \"failOnDataLoss\" to \"false\".\n    "
     ]
    }
   ],
   "source": [
    "spark.streams.awaitAnyTermination()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "query = Kafka_stream_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"task1\") \\\n",
    "    .option(\"partition.assignment.strategy\", \"range\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "query.awaitTermination()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   cnt|\n",
      "+------+\n",
      "|525461|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) AS cnt FROM task1\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.status"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading data incrementally"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark_jars = \"../jars/*\"\n",
    "\n",
    "spark_packages = [\n",
    "    'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2',\n",
    "    'org.apache.kafka:kafka-clients:3.2.3'\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Dataframe using a JDBC Connection\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", spark_jars) \\\n",
    "    .config(\"spark.executor.extraClassPath\", spark_jars) \\\n",
    "    .config(\"spark.jars.packages\", \",\".join(spark_packages)) \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "kafka_df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"topic_data\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"kafka.group.id\", \"incConsumerGroup\") \\\n",
    "    .load()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType\n",
    "from pyspark.sql.functions import col, from_json, to_date\n",
    "\n",
    "eventSchema = StructType() \\\n",
    "    .add('InvoiceNo', StringType()) \\\n",
    "    .add('StockCode', StringType()) \\\n",
    "    .add('Description', StringType()) \\\n",
    "    .add('Quantity', IntegerType()) \\\n",
    "    .add('InvoiceDate', StringType()) \\\n",
    "    .add('UnitPrice', DoubleType()) \\\n",
    "    .add('CustomerID', IntegerType()) \\\n",
    "    .add('Country', StringType())\n",
    "\n",
    "retail_df = kafka_df.select(from_json(col(\"value\"). cast(StringType()), eventSchema).alias(\"message\"), col(\"timestamp\").alias(\"EventTime\")).select(\"message.*\", \"EventTime\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.streaming.StreamingQuery at 0x12b734100>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = \"../data-lake/retail_events-partition\"\n",
    "retail_df.withColumn(\"EventDate\", to_date(retail_df.EventTime)) \\\n",
    "    .writeStream \\\n",
    "    .format('parquet') \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .partitionBy(\"InvoiceDate\",\"Country\") \\\n",
    "    .trigger(once=True) \\\n",
    "    .option('checkpointLocation', base_path + '/_checkpoint') \\\n",
    "    .start(base_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrameReader' object has no attribute 'partitionBy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-13-43a8ebb484ab>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mwritten_retail_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"parquet\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m     \u001B[0;34m.\u001B[0m\u001B[0mpartitionBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"InvoiceDate\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"Country\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbase_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'DataFrameReader' object has no attribute 'partitionBy'"
     ]
    }
   ],
   "source": [
    "written_retail_df = spark.read.format(\"parquet\") \\\n",
    "    .(\"InvoiceDate\",\"Country\") \\\n",
    "    .load(base_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+--------------------+----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|           EventTime| EventDate|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+--------------------+----------+\n",
      "|   503782|    22331|WOODLAND PARTY BA...|      16|07/04/10 11:59|     1.65|     12711|       Germany|2023-03-08 02:13:...|2023-03-08|\n",
      "|   489434|    85048|15CM CHRISTMAS GL...|      12|01/12/09 07:45|     6.95|     13085|United Kingdom|2023-03-08 02:13:...|2023-03-08|\n",
      "|   523645|    21790|  VINTAGE SNAP CARDS|       5|23/09/10 12:19|     0.85|     12838|United Kingdom|2023-03-08 02:13:...|2023-03-08|\n",
      "|   494014|    20714|    POSY SHOPPER BAG|      10|11/01/10 09:33|     0.87|      null|United Kingdom|2023-03-08 02:13:...|2023-03-08|\n",
      "|   513647|    21975|PACK OF 60 DINOSA...|     120|28/06/10 08:28|     0.42|     14646|   Netherlands|2023-03-08 02:13:...|2023-03-08|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "written_retail_df.show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
